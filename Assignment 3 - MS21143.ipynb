{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af7c69c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "0b13adcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "\n",
    "def load(filepath):\n",
    "    x = []\n",
    "    y = []\n",
    "    file = open(filepath,\"r\")\n",
    "    next(file)\n",
    "    for line in file:\n",
    "        digits = line.strip().split(\",\")\n",
    "        y.append(float(digits[0]))\n",
    "        xvals = []\n",
    "        for i in range(1,len(digits)):\n",
    "            vals = float(digits[i])/255\n",
    "            xvals.append(vals)\n",
    "        x.append(xvals)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x,y\n",
    "\n",
    "x,y = load(\"mnist_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "b0ec8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Network object of 0 hidden layers\n",
    "\n",
    "# Problem 2 \n",
    "\n",
    "class Network:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass \n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1.0/(1 + np.exp(-x))\n",
    "        \n",
    "    def softmax(self,x):\n",
    "        num = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return num/ np.sum(num, axis=1, keepdims=True)\n",
    "\n",
    "    def feedforward(self, a,weights,biases):\n",
    "            z = self.softmax(np.dot(a,weights)+biases)\n",
    "            return z\n",
    "    \n",
    "    def backpropagate(self,a,y,lamda,weights,biases):\n",
    "        delta = a - y\n",
    "        del_weights = np.dot(x.T,delta)/np.array(x).shape[0]\n",
    "        del_biases = np.sum(delta)/np.array(x).shape[0]\n",
    "        \n",
    "        weights -= lamda*del_weights\n",
    "        biases -= lamda*del_biases\n",
    "        \n",
    "    def trainingy(self,y):\n",
    "        trainingy = []\n",
    "        for i in y:\n",
    "            onezlist = []\n",
    "            for j in range(10):\n",
    "                if i ==j:\n",
    "                    onezlist.append(1)\n",
    "                else:\n",
    "                    onezlist.append(0)\n",
    "            trainingy.append(onezlist)\n",
    "        trainingy = np.array(trainingy)\n",
    "        return trainingy\n",
    "\n",
    "    \n",
    "    def train(self,x,y,lamda,weights):\n",
    "        biases = np.random.randn(1,10) \n",
    "        y_train = self.trainingy(y)\n",
    "        for i in range(100):\n",
    "            a = self.feedforward(x,weights,biases)\n",
    "            self.backpropagate(a,y_train,lamda,weights,biases)\n",
    "            score = self.accuracy(x,y,weights,biases)\n",
    "            print(f\"Run {i}:{score}\")\n",
    "        return weights,biases,score\n",
    "    \n",
    "    def accuracy(self,x,y,weights,biases):\n",
    "        a = self.feedforward(x,weights,biases)\n",
    "        y_pred = np.argmax(a,axis = 1)\n",
    "        score = np.mean(y == y_pred)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "f0b73880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0:0.29973333333333335\n",
      "Run 1:0.34081666666666666\n",
      "Run 2:0.41336666666666666\n",
      "Run 3:0.461\n",
      "Run 4:0.44576666666666664\n",
      "Run 5:0.5070833333333333\n",
      "Run 6:0.49946666666666667\n",
      "Run 7:0.6188333333333333\n",
      "Run 8:0.6327833333333334\n",
      "Run 9:0.6574166666666666\n",
      "Run 10:0.7134166666666667\n",
      "Run 11:0.7304666666666667\n",
      "Run 12:0.7504\n",
      "Run 13:0.7349166666666667\n",
      "Run 14:0.7117166666666667\n",
      "Run 15:0.7337\n",
      "Run 16:0.78115\n",
      "Run 17:0.8015166666666667\n",
      "Run 18:0.7946\n",
      "Run 19:0.79195\n",
      "Run 20:0.778\n",
      "Run 21:0.7689166666666667\n",
      "Run 22:0.7589166666666667\n",
      "Run 23:0.75135\n",
      "Run 24:0.74365\n",
      "Run 25:0.7421666666666666\n",
      "Run 26:0.7481333333333333\n",
      "Run 27:0.7544833333333333\n",
      "Run 28:0.7716833333333334\n",
      "Run 29:0.7825833333333333\n",
      "Run 30:0.7950833333333334\n",
      "Run 31:0.8082666666666667\n",
      "Run 32:0.81945\n",
      "Run 33:0.8306166666666667\n",
      "Run 34:0.8352333333333334\n",
      "Run 35:0.8398666666666667\n",
      "Run 36:0.8413\n",
      "Run 37:0.8430833333333333\n",
      "Run 38:0.8443666666666667\n",
      "Run 39:0.8444166666666667\n",
      "Run 40:0.8452166666666666\n",
      "Run 41:0.84405\n",
      "Run 42:0.84435\n",
      "Run 43:0.84275\n",
      "Run 44:0.84175\n",
      "Run 45:0.8390666666666666\n",
      "Run 46:0.83675\n",
      "Run 47:0.8327166666666667\n",
      "Run 48:0.82825\n",
      "Run 49:0.8223166666666667\n",
      "Run 50:0.8173833333333334\n",
      "Run 51:0.8129833333333333\n",
      "Run 52:0.8102333333333334\n",
      "Run 53:0.8127666666666666\n",
      "Run 54:0.814\n",
      "Run 55:0.8220166666666666\n",
      "Run 56:0.8265833333333333\n",
      "Run 57:0.8332166666666667\n",
      "Run 58:0.8366\n",
      "Run 59:0.8382\n",
      "Run 60:0.8369666666666666\n",
      "Run 61:0.8306166666666667\n",
      "Run 62:0.8203166666666667\n",
      "Run 63:0.8064666666666667\n",
      "Run 64:0.7868\n",
      "Run 65:0.7916333333333333\n",
      "Run 66:0.8176833333333333\n",
      "Run 67:0.8249833333333333\n",
      "Run 68:0.8320833333333333\n",
      "Run 69:0.8398833333333333\n",
      "Run 70:0.8452166666666666\n",
      "Run 71:0.8514833333333334\n",
      "Run 72:0.8485333333333334\n",
      "Run 73:0.8498666666666667\n",
      "Run 74:0.8429333333333333\n",
      "Run 75:0.84015\n",
      "Run 76:0.8318\n",
      "Run 77:0.82625\n",
      "Run 78:0.8198833333333333\n",
      "Run 79:0.8174166666666667\n",
      "Run 80:0.8171333333333334\n",
      "Run 81:0.8262833333333334\n",
      "Run 82:0.8317\n",
      "Run 83:0.84605\n",
      "Run 84:0.85415\n",
      "Run 85:0.8659166666666667\n",
      "Run 86:0.871\n",
      "Run 87:0.87445\n",
      "Run 88:0.8771333333333333\n",
      "Run 89:0.8771666666666667\n",
      "Run 90:0.87895\n",
      "Run 91:0.8788666666666667\n",
      "Run 92:0.8797666666666667\n",
      "Run 93:0.8798\n",
      "Run 94:0.8804666666666666\n",
      "Run 95:0.8804833333333333\n",
      "Run 96:0.88105\n",
      "Run 97:0.8808666666666667\n",
      "Run 98:0.8816333333333334\n",
      "Run 99:0.8814166666666666\n"
     ]
    }
   ],
   "source": [
    "net = Network()\n",
    "weights = np.random.randn(784,10)\n",
    "w,b,s =  net.train(x,y,5,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "06cc59df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Network object of n layers of n dimension\n",
    "\n",
    "# General solution for problem 3,4.\n",
    "\n",
    "# This object can be used to create Neural Networks of any number of layers\n",
    "# and each layer having any number of nodes\n",
    "# weights and biases are fed into the class\n",
    "#learning rate and epochs can be changed when we call the function.\n",
    "class Network2:\n",
    "    \n",
    "    def __init__(self,biases,weights):\n",
    "        \n",
    "        self.biases = biases\n",
    "        self.weights = weights\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        return 1.0/(1 + np.exp(-x))\n",
    "        \n",
    "    def softmax(self,x):\n",
    "        num = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return num/ np.sum(num, axis=1, keepdims=True)\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        alist = []\n",
    "        alist.append(a)\n",
    "        for i in range(len(self.biases)):\n",
    "            if i == len(self.biases) -1:\n",
    "                z = self.softmax(np.dot(a,self.weights[i])+self.biases[i])\n",
    "                alist.append(z)\n",
    "                a = z\n",
    "                return z,alist\n",
    "            else:\n",
    "                z = self.sigmoid(np.dot(a,self.weights[i])+self.biases[i])\n",
    "                a = z\n",
    "                alist.append(z)\n",
    "                \n",
    "    def trainingy(self,y):\n",
    "        trainingy = []\n",
    "        for i in y:\n",
    "            onezlist = []\n",
    "            for j in range(10):\n",
    "                if i ==j:\n",
    "                    onezlist.append(1)\n",
    "                else:\n",
    "                    onezlist.append(0)\n",
    "            trainingy.append(onezlist)\n",
    "        trainingy = np.array(trainingy)\n",
    "        return trainingy\n",
    "        \n",
    "                \n",
    "    def backpropagate(self,alist,y,lamda):\n",
    "        delta_prev = 0\n",
    "        for i in range(len(self.biases)):\n",
    "            a = alist[len(alist) - 1 - i]\n",
    "            prev_a = alist[len(alist)- 2 - i]\n",
    "            if i == 0:\n",
    "                delta = a - y\n",
    "                delta_prev = delta\n",
    "                del_weights = np.dot(prev_a.T,delta)/x.shape[0]\n",
    "                del_biases = np.sum(delta,axis = 0)/x.shape[0]\n",
    "                \n",
    "                self.weights[len(self.biases) -1 -i] -= lamda*del_weights\n",
    "                self.biases[len(self.biases) -1 -i] -= lamda*del_biases\n",
    "            else:\n",
    "                dau = np.dot(delta_prev,self.weights[len(self.biases)-i].T)\n",
    "                delta = dau*(a*(1-a))\n",
    "                delta_prev = delta\n",
    "                del_weights = np.dot(prev_a.T,delta)/x.shape[0]\n",
    "                del_biases = np.sum(delta,axis = 0)/x.shape[0]\n",
    "\n",
    "                self.weights[len(self.biases) -1 -i] -= lamda*del_weights\n",
    "                self.biases[len(self.biases) -1 -i] -= lamda*del_biases\n",
    "                \n",
    "\n",
    "    \n",
    "    def train(self,x,y,lamda):\n",
    "        y_train = self.trainingy(y)\n",
    "        for i in range(100):\n",
    "            a,alist = self.feedforward(x)\n",
    "            self.backpropagate(alist,y_train,lamda)\n",
    "            score = self.accuracy(x,y)\n",
    "            print(f\"Run {i}:{score}\")\n",
    "        return self.weights,self.biases,score\n",
    "    \n",
    "    def accuracy(self,x,y):\n",
    "        a,alist = self.feedforward(x)\n",
    "        y_pred = np.argmax(a,axis = 1)\n",
    "        score = np.mean(y == y_pred)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "277ede34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0:0.21358333333333332\n",
      "Run 1:0.2713\n",
      "Run 2:0.359\n",
      "Run 3:0.384\n",
      "Run 4:0.4596166666666667\n",
      "Run 5:0.4863166666666667\n",
      "Run 6:0.5285833333333333\n",
      "Run 7:0.5483\n",
      "Run 8:0.5745666666666667\n",
      "Run 9:0.5880166666666666\n",
      "Run 10:0.6081166666666666\n",
      "Run 11:0.61785\n",
      "Run 12:0.6336333333333334\n",
      "Run 13:0.6423166666666666\n",
      "Run 14:0.6549833333333334\n",
      "Run 15:0.6637\n",
      "Run 16:0.6744166666666667\n",
      "Run 17:0.6816666666666666\n",
      "Run 18:0.6905833333333333\n",
      "Run 19:0.6968333333333333\n",
      "Run 20:0.7054833333333334\n",
      "Run 21:0.71115\n",
      "Run 22:0.7184\n",
      "Run 23:0.7233833333333334\n",
      "Run 24:0.7294666666666667\n",
      "Run 25:0.7340166666666667\n",
      "Run 26:0.7389\n",
      "Run 27:0.7427\n",
      "Run 28:0.74755\n",
      "Run 29:0.75055\n",
      "Run 30:0.7552333333333333\n",
      "Run 31:0.7578\n",
      "Run 32:0.7614166666666666\n",
      "Run 33:0.7639666666666667\n",
      "Run 34:0.7671166666666667\n",
      "Run 35:0.7698\n",
      "Run 36:0.7721166666666667\n",
      "Run 37:0.7743166666666667\n",
      "Run 38:0.77715\n",
      "Run 39:0.7793666666666667\n",
      "Run 40:0.78175\n",
      "Run 41:0.784\n",
      "Run 42:0.7863\n",
      "Run 43:0.7881833333333333\n",
      "Run 44:0.7897666666666666\n",
      "Run 45:0.7917\n",
      "Run 46:0.7933333333333333\n",
      "Run 47:0.7950333333333334\n",
      "Run 48:0.7969166666666667\n",
      "Run 49:0.7983\n",
      "Run 50:0.7998666666666666\n",
      "Run 51:0.8012\n",
      "Run 52:0.8024166666666667\n",
      "Run 53:0.8036166666666666\n",
      "Run 54:0.8052833333333334\n",
      "Run 55:0.8065833333333333\n",
      "Run 56:0.80795\n",
      "Run 57:0.80935\n",
      "Run 58:0.81035\n",
      "Run 59:0.8114\n",
      "Run 60:0.8124\n",
      "Run 61:0.8137333333333333\n",
      "Run 62:0.81485\n",
      "Run 63:0.8158\n",
      "Run 64:0.8167333333333333\n",
      "Run 65:0.8175833333333333\n",
      "Run 66:0.8185333333333333\n",
      "Run 67:0.8195333333333333\n",
      "Run 68:0.82045\n",
      "Run 69:0.82135\n",
      "Run 70:0.8221666666666667\n",
      "Run 71:0.82305\n",
      "Run 72:0.82375\n",
      "Run 73:0.8244666666666667\n",
      "Run 74:0.8252166666666667\n",
      "Run 75:0.82605\n",
      "Run 76:0.8269333333333333\n",
      "Run 77:0.8276666666666667\n",
      "Run 78:0.8285\n",
      "Run 79:0.8292166666666667\n",
      "Run 80:0.8299\n",
      "Run 81:0.8306833333333333\n",
      "Run 82:0.8312666666666667\n",
      "Run 83:0.832\n",
      "Run 84:0.8326833333333333\n",
      "Run 85:0.8335333333333333\n",
      "Run 86:0.8342833333333334\n",
      "Run 87:0.8348666666666666\n",
      "Run 88:0.8355333333333334\n",
      "Run 89:0.8358833333333333\n",
      "Run 90:0.8364833333333334\n",
      "Run 91:0.8369833333333333\n",
      "Run 92:0.8377666666666667\n",
      "Run 93:0.8385333333333334\n",
      "Run 94:0.8392166666666667\n",
      "Run 95:0.8397833333333333\n",
      "Run 96:0.8404666666666667\n",
      "Run 97:0.8410333333333333\n",
      "Run 98:0.8415333333333334\n",
      "Run 99:0.8421333333333333\n"
     ]
    }
   ],
   "source": [
    "# Problem 3 \n",
    "# 1 Hidden Layer with 10 nodes\n",
    "biases = [np.random.randn(1,10)]\n",
    "weights = [np.random.randn(784,10)]\n",
    "net = Network2(biases,weights)\n",
    "w,b,s =  net.train(x,y,2)\n",
    "\n",
    "#train(input,output,learning rate,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "1df2600b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0:0.17225\n",
      "Run 1:0.18603333333333333\n",
      "Run 2:0.20846666666666666\n",
      "Run 3:0.23346666666666666\n",
      "Run 4:0.2836166666666667\n",
      "Run 5:0.3202\n",
      "Run 6:0.34291666666666665\n",
      "Run 7:0.3658\n",
      "Run 8:0.38425\n",
      "Run 9:0.4020166666666667\n",
      "Run 10:0.41688333333333333\n",
      "Run 11:0.43061666666666665\n",
      "Run 12:0.44376666666666664\n",
      "Run 13:0.4559666666666667\n",
      "Run 14:0.4680666666666667\n",
      "Run 15:0.4792\n",
      "Run 16:0.49078333333333335\n",
      "Run 17:0.5018\n",
      "Run 18:0.5111833333333333\n",
      "Run 19:0.52025\n",
      "Run 20:0.5300166666666667\n",
      "Run 21:0.5389333333333334\n",
      "Run 22:0.548\n",
      "Run 23:0.5562833333333334\n",
      "Run 24:0.5641\n",
      "Run 25:0.5720666666666666\n",
      "Run 26:0.5801\n",
      "Run 27:0.5869666666666666\n",
      "Run 28:0.59475\n",
      "Run 29:0.6019166666666667\n",
      "Run 30:0.6085\n",
      "Run 31:0.61425\n",
      "Run 32:0.6204\n",
      "Run 33:0.6255833333333334\n",
      "Run 34:0.63015\n",
      "Run 35:0.6352666666666666\n",
      "Run 36:0.6400833333333333\n",
      "Run 37:0.6455833333333333\n",
      "Run 38:0.6502333333333333\n",
      "Run 39:0.6546\n",
      "Run 40:0.6589666666666667\n",
      "Run 41:0.6627833333333333\n",
      "Run 42:0.66765\n",
      "Run 43:0.6712166666666667\n",
      "Run 44:0.6739666666666667\n",
      "Run 45:0.6774\n",
      "Run 46:0.6807333333333333\n",
      "Run 47:0.68375\n",
      "Run 48:0.6868166666666666\n",
      "Run 49:0.6897\n",
      "Run 50:0.6928166666666666\n",
      "Run 51:0.69575\n",
      "Run 52:0.6983833333333334\n",
      "Run 53:0.70135\n",
      "Run 54:0.7035833333333333\n",
      "Run 55:0.7066833333333333\n",
      "Run 56:0.7091166666666666\n",
      "Run 57:0.7115333333333334\n",
      "Run 58:0.7137\n",
      "Run 59:0.7158833333333333\n",
      "Run 60:0.7181666666666666\n",
      "Run 61:0.7201\n",
      "Run 62:0.7223333333333334\n",
      "Run 63:0.7243833333333334\n",
      "Run 64:0.7265666666666667\n",
      "Run 65:0.7283666666666667\n",
      "Run 66:0.7298833333333333\n",
      "Run 67:0.7317\n",
      "Run 68:0.7331833333333333\n",
      "Run 69:0.7349\n",
      "Run 70:0.7368666666666667\n",
      "Run 71:0.7385666666666667\n",
      "Run 72:0.7401166666666666\n",
      "Run 73:0.7413666666666666\n",
      "Run 74:0.7432333333333333\n",
      "Run 75:0.74515\n",
      "Run 76:0.74665\n",
      "Run 77:0.7478833333333333\n",
      "Run 78:0.74945\n",
      "Run 79:0.7506\n",
      "Run 80:0.7520833333333333\n",
      "Run 81:0.75385\n",
      "Run 82:0.75525\n",
      "Run 83:0.7563666666666666\n",
      "Run 84:0.7576833333333334\n",
      "Run 85:0.7587166666666667\n",
      "Run 86:0.7599833333333333\n",
      "Run 87:0.7613666666666666\n",
      "Run 88:0.7627\n",
      "Run 89:0.764\n",
      "Run 90:0.7653166666666666\n",
      "Run 91:0.76665\n",
      "Run 92:0.76775\n",
      "Run 93:0.76885\n",
      "Run 94:0.7702166666666667\n",
      "Run 95:0.7718333333333334\n",
      "Run 96:0.7733\n",
      "Run 97:0.77445\n",
      "Run 98:0.77535\n",
      "Run 99:0.77665\n"
     ]
    }
   ],
   "source": [
    "# Problem 4\n",
    "# Multilayered Network with 4 Layers, Hidden Layers with 10 nodes each\n",
    "biases = [np.random.randn(1,10),np.random.randn(1,10)]\n",
    "weights = [np.random.randn(784,10),np.random.randn(10,10)]\n",
    "net = Network2(biases,weights)\n",
    "w,b,s = net.train(x,y,3)\n",
    "#train(input,output,learning rate,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fc16dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Network with Different Activation Function\n",
    "# Network Object can take any number of layers\n",
    "# Each Layer can take any number of nodes\n",
    "class NetworkAct:\n",
    "    \n",
    "    def __init__(self,biases,weights,actfunc):\n",
    "        self.biases = biases\n",
    "        self.weights = weights\n",
    "        self.actfunc = actfunc\n",
    "        \n",
    "        \n",
    "    def relu(self,x):\n",
    "        return np.maximum(0,x)\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        return 1.0/(1 + np.exp(-x))\n",
    "        \n",
    "    def softmax(self,x):\n",
    "        num = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return num/ np.sum(num, axis=1, keepdims=True)\n",
    "    \n",
    "    def tanh(self,x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        alist = []\n",
    "        alist.append(a)\n",
    "        for i in range(len(self.biases)):\n",
    "            if i == len(self.biases) -1:\n",
    "                z = self.softmax(np.dot(a,self.weights[i])+self.biases[i])\n",
    "                alist.append(z)\n",
    "                a = z\n",
    "                return z,alist\n",
    "            else:\n",
    "                if self.actfunc == \"relu\":\n",
    "                    z = self.relu(np.dot(a,self.weights[i])+self.biases[i])\n",
    "                if self.actfunc == \"sigmoid\":\n",
    "                    z = self.sigmoid(np.dot(a,self.weights[i])+self.biases[i])\n",
    "                if self.actfunc == \"tanh\":\n",
    "                    z = self.tanh(np.dot(a,self.weights[i])+self.biases[i])\n",
    "                a = z\n",
    "                alist.append(z)\n",
    "                \n",
    "    def trainingy(self,y):\n",
    "        trainingy = []\n",
    "        for i in y:\n",
    "            onezlist = []\n",
    "            for j in range(10):\n",
    "                if i ==j:\n",
    "                    onezlist.append(1)\n",
    "                else:\n",
    "                    onezlist.append(0)\n",
    "            trainingy.append(onezlist)\n",
    "        trainingy = np.array(trainingy)\n",
    "        return trainingy\n",
    "        \n",
    "                \n",
    "    def backpropagate(self,alist,y,lamda):\n",
    "        delta_prev = 0\n",
    "        for i in range(len(self.biases)):\n",
    "            a = alist[len(alist) - 1 - i]\n",
    "            prev_a = alist[len(alist)- 2 - i]\n",
    "            if i == 0:\n",
    "                delta = a - y\n",
    "                delta_prev = delta\n",
    "                del_weights = np.dot(prev_a.T,delta)/x.shape[0]\n",
    "                del_biases = np.sum(delta,axis = 0)/x.shape[0]\n",
    "                \n",
    "                self.weights[len(self.biases) -1 -i] -= lamda*del_weights\n",
    "                self.biases[len(self.biases) -1 -i] -= lamda*del_biases\n",
    "            else:\n",
    "                dau = np.dot(delta_prev,self.weights[len(self.biases)-i].T)\n",
    "                if self.actfunc == \"relu\":\n",
    "                    delta = dau*(a>0)\n",
    "                if self.actfunc == \"sigmoid\":\n",
    "                    delta = dau*(a*(1-a))\n",
    "                if self.actfunc == \"tanh\":\n",
    "                    delta = dau*(1 - a**2)\n",
    "                delta_prev = delta\n",
    "                del_weights = np.dot(prev_a.T,delta)/x.shape[0]\n",
    "                del_biases = np.sum(delta,axis = 0)/x.shape[0]\n",
    "\n",
    "                self.weights[len(self.biases) -1 -i] -= lamda*del_weights\n",
    "                self.biases[len(self.biases) -1 -i] -= lamda*del_biases\n",
    "                \n",
    "\n",
    "    \n",
    "    def train(self,x,y,lamda):\n",
    "        y_train = self.trainingy(y)\n",
    "        for i in range(100):\n",
    "            a,alist = self.feedforward(x)\n",
    "            self.backpropagate(alist,y_train,lamda)\n",
    "            score = self.accuracy(x,y)\n",
    "            print(f\"Run {i}:{score}\")\n",
    "        return self.weights,self.biases,score\n",
    "    \n",
    "    def accuracy(self,x,y):\n",
    "        a,alist = self.feedforward(x)\n",
    "        y_pred = np.argmax(a,axis = 1)\n",
    "        score = np.mean(y == y_pred)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1f1d95f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0:0.09751666666666667\n",
      "Run 1:0.0993\n",
      "Run 2:0.09871666666666666\n",
      "Run 3:0.09751666666666667\n",
      "Run 4:0.10218333333333333\n",
      "Run 5:0.09871666666666666\n",
      "Run 6:0.09751666666666667\n",
      "Run 7:0.10218333333333333\n",
      "Run 8:0.09871666666666666\n",
      "Run 9:0.09751666666666667\n",
      "Run 10:0.10218333333333333\n",
      "Run 11:0.09871666666666666\n",
      "Run 12:0.09751666666666667\n",
      "Run 13:0.10218333333333333\n",
      "Run 14:0.09871666666666666\n",
      "Run 15:0.09751666666666667\n",
      "Run 16:0.10218333333333333\n",
      "Run 17:0.09871666666666666\n",
      "Run 18:0.09751666666666667\n",
      "Run 19:0.10218333333333333\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m weights \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m784\u001b[39m,\u001b[38;5;241m10\u001b[39m),np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m10\u001b[39m)]\n\u001b[0;32m      4\u001b[0m net \u001b[38;5;241m=\u001b[39m NetworkAct(biases,weights,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m w,b,s \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 91\u001b[0m, in \u001b[0;36mNetworkAct.train\u001b[1;34m(self, x, y, lamda)\u001b[0m\n\u001b[0;32m     89\u001b[0m y_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainingy(y)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m---> 91\u001b[0m     a,alist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeedforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackpropagate(alist,y_train,lamda)\n\u001b[0;32m     93\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccuracy(x,y)\n",
      "Cell \u001b[1;32mIn[10], line 36\u001b[0m, in \u001b[0;36mNetworkAct.feedforward\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactfunc \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 36\u001b[0m         z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[i])\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactfunc \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     38\u001b[0m         z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(np\u001b[38;5;241m.\u001b[39mdot(a,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[i])\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[i])\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Problem 5\n",
    "biases = [np.random.randn(1,10),np.random.randn(1,10)]\n",
    "weights = [np.random.randn(784,10),np.random.randn(10,10)]\n",
    "net = NetworkAct(biases,weights,\"relu\")\n",
    "w,b,s = net.train(x,y,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "29842b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Network with Different Activation Function\n",
    "# Weights, Biases are passed to the class\n",
    "# Each Layer can take any number of nodes\n",
    "class NetworkMomentum:\n",
    "    \n",
    "    def __init__(self,biases,weights,actfunc):\n",
    "        self.biases = biases\n",
    "        self.weights = weights\n",
    "        self.actfunc = actfunc\n",
    "        self.biasadder = [np.zeros_like(i) for i in self.biases]\n",
    "        self.weightadder = [np.zeros_like(i) for i in self.weights ]\n",
    "        \n",
    "\n",
    "        \n",
    "    def relu(self,x):\n",
    "        return np.maximum(0,x)\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        return 1.0/(1 + np.exp(-x))\n",
    "        \n",
    "    def softmax(self,x):\n",
    "        num = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return num/ np.sum(num, axis=1, keepdims=True)\n",
    "    \n",
    "    def tanh(self,x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        alist = []\n",
    "        alist.append(a)\n",
    "        for i in range(len(self.biases)):\n",
    "            if i == len(self.biases) -1:\n",
    "                z = self.softmax(np.dot(a,self.weights[i])+self.biases[i])\n",
    "                alist.append(z)\n",
    "                a = z\n",
    "                return z,alist\n",
    "            else:\n",
    "                if self.actfunc == \"relu\":\n",
    "                    z = self.relu(np.dot(a,self.weights[i])+self.biases[i])\n",
    "                if self.actfunc == \"sigmoid\":\n",
    "                    z = self.sigmoid(np.dot(a,self.weights[i])+self.biases[i])\n",
    "                if self.actfunc == \"tanh\":\n",
    "                    z = self.tanh(np.dot(a,self.weights[i])+self.biases[i])\n",
    "                a = z\n",
    "                alist.append(z)\n",
    "                \n",
    "    def trainingy(self,y):\n",
    "        trainingy = []\n",
    "        for i in y:\n",
    "            onezlist = []\n",
    "            for j in range(10):\n",
    "                if i ==j:\n",
    "                    onezlist.append(1)\n",
    "                else:\n",
    "                    onezlist.append(0)\n",
    "            trainingy.append(onezlist)\n",
    "        trainingy = np.array(trainingy)\n",
    "        return trainingy\n",
    "        \n",
    "                \n",
    "    def backpropagate(self,alist,y,lamda,momentum):\n",
    "        delta_prev = 0\n",
    "        for i in range(len(self.biases)):\n",
    "            a = alist[len(alist) - 1 - i]\n",
    "            prev_a = alist[len(alist)- 2 - i]\n",
    "            if i == 0:\n",
    "                delta = a - y\n",
    "                delta_prev = delta\n",
    "                del_weights = np.dot(prev_a.T,delta)/x.shape[0]\n",
    "                del_biases = np.sum(delta,axis = 0)/x.shape[0]\n",
    "                \n",
    "                self.weightadder[len(self.biases) - 1 -i] = momentum*self.weightadder[len(self.biases) -1 -i] - lamda*del_weights\n",
    "                self.biasadder[len(self.biases) - 1 -i] = momentum*self.biasadder[len(self.biases) -1 -i] - lamda*del_biases\n",
    "                self.weights[len(self.biases) -1 -i] += self.weightadder[len(self.biases)-1-i]\n",
    "                self.biases[len(self.biases) -1 -i] += self.biasadder[len(self.biases)-1-i]\n",
    "            else:\n",
    "                dau = np.dot(delta_prev,self.weights[len(self.biases)-i].T)\n",
    "                if self.actfunc == \"relu\":\n",
    "                    delta = dau*(a>0)\n",
    "                if self.actfunc == \"sigmoid\":\n",
    "                    delta = dau*(a*(1-a))\n",
    "                if self.actfunc == \"tanh\":\n",
    "                    delta = dau*(1 - a**2)\n",
    "                delta_prev = delta\n",
    "                del_weights = np.dot(prev_a.T,delta)/x.shape[0]\n",
    "                del_biases = np.sum(delta,axis = 0)/x.shape[0]\n",
    "\n",
    "\n",
    "                \n",
    "                self.weightadder[len(self.biases) - 1 -i] = momentum*self.weightadder[len(self.biases) -1 -i] - lamda*del_weights\n",
    "                self.biasadder[len(self.biases) - 1 -i] = momentum*self.biasadder[len(self.biases) -1 -i] - lamda*del_biases\n",
    "                self.weights[len(self.biases) -1 -i] += self.weightadder[len(self.biases)-1-i]\n",
    "                self.biases[len(self.biases) -1 -i] += self.biasadder[len(self.biases)-1-i]\n",
    "                \n",
    "\n",
    "    \n",
    "    def train(self,x,y,lamda,momentum,epoch):\n",
    "        y_train = self.trainingy(y)\n",
    "        for i in range(epoch):\n",
    "            a,alist = self.feedforward(x)\n",
    "            self.backpropagate(alist,y_train,lamda,momentum)\n",
    "            score = self.accuracy(x,y)\n",
    "            print(f\"Run {i}:{score}\")\n",
    "        return self.weights,self.biases,score\n",
    "    \n",
    "    def accuracy(self,x,y):\n",
    "        a,alist = self.feedforward(x)\n",
    "        y_pred = np.argmax(a,axis = 1)\n",
    "        score = np.mean(y == y_pred)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "f89baf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0:0.27098333333333335\n",
      "Run 1:0.3368\n",
      "Run 2:0.4029333333333333\n",
      "Run 3:0.4323666666666667\n",
      "Run 4:0.4587333333333333\n",
      "Run 5:0.47258333333333336\n",
      "Run 6:0.4879833333333333\n",
      "Run 7:0.5035166666666666\n",
      "Run 8:0.5170166666666667\n",
      "Run 9:0.5265166666666666\n",
      "Run 10:0.5363666666666667\n",
      "Run 11:0.5447833333333333\n",
      "Run 12:0.5544166666666667\n",
      "Run 13:0.5625166666666667\n",
      "Run 14:0.5699833333333333\n",
      "Run 15:0.5781\n",
      "Run 16:0.5858333333333333\n",
      "Run 17:0.5927333333333333\n",
      "Run 18:0.5995833333333334\n",
      "Run 19:0.6046666666666667\n",
      "Run 20:0.6105\n",
      "Run 21:0.6151333333333333\n",
      "Run 22:0.6200833333333333\n",
      "Run 23:0.62495\n",
      "Run 24:0.62935\n",
      "Run 25:0.63325\n",
      "Run 26:0.639\n",
      "Run 27:0.6418333333333334\n",
      "Run 28:0.64505\n",
      "Run 29:0.6480333333333334\n",
      "Run 30:0.6513666666666666\n",
      "Run 31:0.6545333333333333\n",
      "Run 32:0.6581166666666667\n",
      "Run 33:0.6609166666666667\n",
      "Run 34:0.6639166666666667\n",
      "Run 35:0.6662666666666667\n",
      "Run 36:0.6699666666666667\n",
      "Run 37:0.6722833333333333\n",
      "Run 38:0.6745666666666666\n",
      "Run 39:0.6766833333333333\n",
      "Run 40:0.6789333333333334\n",
      "Run 41:0.6809\n",
      "Run 42:0.6834166666666667\n",
      "Run 43:0.6858666666666666\n",
      "Run 44:0.6876833333333333\n",
      "Run 45:0.6898166666666666\n",
      "Run 46:0.6918666666666666\n",
      "Run 47:0.6937666666666666\n",
      "Run 48:0.6959\n",
      "Run 49:0.6980333333333333\n",
      "Run 50:0.70015\n",
      "Run 51:0.7024666666666667\n",
      "Run 52:0.7044833333333334\n",
      "Run 53:0.7064833333333334\n",
      "Run 54:0.7086833333333333\n",
      "Run 55:0.71055\n",
      "Run 56:0.71275\n",
      "Run 57:0.7143666666666667\n",
      "Run 58:0.71615\n",
      "Run 59:0.7178\n",
      "Run 60:0.7193666666666667\n",
      "Run 61:0.72085\n",
      "Run 62:0.72265\n",
      "Run 63:0.7246833333333333\n",
      "Run 64:0.7263333333333334\n",
      "Run 65:0.7276666666666667\n",
      "Run 66:0.7289166666666667\n",
      "Run 67:0.7309166666666667\n",
      "Run 68:0.73305\n",
      "Run 69:0.7344333333333334\n",
      "Run 70:0.7355666666666667\n",
      "Run 71:0.7371\n",
      "Run 72:0.7382833333333333\n",
      "Run 73:0.7397666666666667\n",
      "Run 74:0.74115\n",
      "Run 75:0.74235\n",
      "Run 76:0.74415\n",
      "Run 77:0.7456333333333334\n",
      "Run 78:0.7467166666666667\n",
      "Run 79:0.74765\n",
      "Run 80:0.74905\n",
      "Run 81:0.7505\n",
      "Run 82:0.7515\n",
      "Run 83:0.7524166666666666\n",
      "Run 84:0.7533166666666666\n",
      "Run 85:0.7540666666666667\n",
      "Run 86:0.7551166666666667\n",
      "Run 87:0.7562333333333333\n",
      "Run 88:0.75745\n",
      "Run 89:0.7587166666666667\n",
      "Run 90:0.7599333333333333\n",
      "Run 91:0.76085\n",
      "Run 92:0.7620333333333333\n",
      "Run 93:0.7631333333333333\n",
      "Run 94:0.7644666666666666\n",
      "Run 95:0.7656333333333334\n",
      "Run 96:0.7667833333333334\n",
      "Run 97:0.7675666666666666\n",
      "Run 98:0.7684666666666666\n",
      "Run 99:0.76965\n"
     ]
    }
   ],
   "source": [
    "#Problem6\n",
    "biases = [np.random.randn(1,10),np.random.randn(1,10)]\n",
    "weights = [np.random.randn(784,10),np.random.randn(10,10)]\n",
    "net = NetworkMomentum(biases,weights,\"tanh\")\n",
    "w,b,s = net.train(x,y,3,0.1,100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
